{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWBIWYbdXoTm"
      },
      "source": [
        "#LLAMA CPP\n",
        "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\n",
        "\n",
        "https://github.com/abetlen/llama-cpp-python\n",
        "\n",
        "Download the model https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q4_K_M.gguf (you can choose a smaller model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCVAORe5J91I",
        "outputId": "4d6e3d98-4258-4107-d849-35c3b45ce641"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os4XqtFKKJYO",
        "outputId": "d862c6b4-1e3a-47ea-c629-caceeb7ff145"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../../Models/LLAMA2/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Put the location of to the GGUF model that you've download from HuggingFace here\n",
        "model_path = \"../../Models/LLAMA2/llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "#you can use a smaller model to speed up prototyping\n",
        "\n",
        "# Create a llama model\n",
        "model = Llama(model_path=model_path,\n",
        "      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      # seed=1337, # Uncomment to set a specific seed\n",
        "      # n_ctx=2048, # Uncomment to increase the context window\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzXn67n1PJk6",
        "outputId": "b3dc1299-1713-4a8a-e8e0-732d64d1299f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =      12.23 ms /   100 runs   (    0.12 ms per token,  8173.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3716.00 ms /    35 tokens (  106.17 ms per token,     9.42 tokens per second)\n",
            "llama_print_timings:        eval time =   17079.78 ms /    99 runs   (  172.52 ms per token,     5.80 tokens per second)\n",
            "llama_print_timings:       total time =   20995.40 ms /   134 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 'cmpl-da1e7473-1ac4-4dae-a314-b11e08aeb0ca',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1713876378,\n",
              " 'model': '../../Models/LLAMA2/llama-2-7b-chat.Q4_K_M.gguf',\n",
              " 'choices': [{'text': \"<s>[INST] <<SYS>>\\nYou are a helpful assistant\\n<</SYS>>\\nGenerate a list of 5 puns [/INST]  Of course, I'd be happy to help! Here are five puns for you:\\n\\n1. Why did the scarecrow win an award? Because he was outstanding in his field!\\n2. What do you call a fake noodle? An impasta!\\n3. Why did the bicycle fall over? Because it was two-tired!\\n4. Why did the chicken cross the playground? To get to the other slide\",\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 35, 'completion_tokens': 100, 'total_tokens': 135}}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Prompt creation\n",
        "system_message = \"You are a helpful assistant\"\n",
        "user_message = \"Generate a list of 5 puns\"\n",
        "\n",
        "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{system_message}\n",
        "<</SYS>>\n",
        "{user_message} [/INST]\"\"\"\n",
        "\n",
        "# Model parameters\n",
        "max_tokens = 100\n",
        "\n",
        "# Run the model\n",
        "output = model(prompt, max_tokens=max_tokens, echo=True)\n",
        "# Generate a completion, can also call create_completion\n",
        "# Echo the prompt back in the output\n",
        "#stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "'''\n",
        "create_completion(prompt, suffix=None, max_tokens=16, temperature=0.8, top_p=0.95,\n",
        "min_p=0.05, typical_p=1.0, logprobs=None, echo=False, stop=[], frequency_penalty=0.0,\n",
        "presence_penalty=0.0, repeat_penalty=1.1, top_k=40, stream=False, seed=None, tfs_z=1.0,\n",
        "mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None, stopping_criteria=None,\n",
        "logits_processor=None, grammar=None, logit_bias=None)\n",
        "'''\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuCLcEbhZRHt",
        "outputId": "d58d5b19-a581-400e-d2e5-55db383ac96c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': \"<s>[INST] <<SYS>>\\nYou are a helpful assistant\\n<</SYS>>\\nGenerate a list of 5 puns [/INST]  Of course, I'd be happy to help! Here are five puns for you:\\n\\n1. Why did the scarecrow win an award? Because he was outstanding in his field!\\n2. What do you call a fake noodle? An impasta!\\n3. Why did the bicycle fall over? Because it was two-tired!\\n4. Why did the chicken cross the playground? To get to the other slide\",\n",
              "  'index': 0,\n",
              "  'logprobs': None,\n",
              "  'finish_reason': 'length'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output['choices']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEeLCLWxaGbn",
        "outputId": "4ba79749-855a-4ae8-df5e-84572bac0355"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =       5.33 ms /    31 runs   (    0.17 ms per token,  5812.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4436.90 ms /    50 tokens (   88.74 ms per token,    11.27 tokens per second)\n",
            "llama_print_timings:        eval time =    3850.11 ms /    30 runs   (  128.34 ms per token,     7.79 tokens per second)\n",
            "llama_print_timings:       total time =    8359.58 ms /    80 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-d6c57bb7-b7fa-4859-9bdd-29ecbc9975e9',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1713876758,\n",
              " 'model': '../../Models/LLAMA2/llama-2-7b-chat.Q4_K_M.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': \"  In today's world of automation and robotics, scientists and industries are working together to develop self-driving cars.\"},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 63, 'completion_tokens': 30, 'total_tokens': 93}}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat=model.create_chat_completion(\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"You are a scientific journalist who popularizes scientific results.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"Simplify the following text:\\nIn the modern era of automation and robotics, autonomous vehicles are currently the focus of academic and industrial research.\"\n",
        "          }\n",
        "      ]\n",
        ")\n",
        "chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "-xurBlrDgJKP",
        "outputId": "127b85c1-1c2b-4f7d-95d2-1d909cc30361"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =       6.87 ms /    39 runs   (    0.18 ms per token,  5680.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    5338.39 ms /    39 runs   (  136.88 ms per token,     7.31 tokens per second)\n",
            "llama_print_timings:       total time =    5426.76 ms /    40 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"As more and more drones are being used in both civilian and commercial settings, there's a growing need for them to be able to operate independently without human intervention.\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def simplify(snt):\n",
        "  c=model.create_chat_completion(\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"You are a scientific journalist who popularizes scientific results.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"Simplify the following text:\\n\"+snt\n",
        "          }\n",
        "      ]\n",
        "  )\n",
        "  return c['choices'][0]['message']['content'].strip()\n",
        "\n",
        "simplify(\"With the ever increasing number of unmanned aerial vehicles getting involved in activities in the civilian and commercial domain, there is an increased need for autonomy in these systems too.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8trPO3iX-H"
      },
      "source": [
        "### Exercise 1: Simplify 5 sentences\n",
        "\n",
        "\n",
        "\n",
        "1.   Load data into a Pandas DataFrame: https://drive.google.com/file/d/1PMmHCXnU5pSO0v1stMwjipwbD7TswOrr/view?usp=drive_link\n",
        "2.   Simplify 5 sentences (data.head(5)) with LLAMA 2 Chat. You can use the function *simplify* or write your own function. You can test the overwriting of the default parameters, e.g. max_tokens=16, temperature=0.8, top_p=0.95, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tHl7htchjY3G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =      18.25 ms /   103 runs   (    0.18 ms per token,  5643.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2778.22 ms /    35 tokens (   79.38 ms per token,    12.60 tokens per second)\n",
            "llama_print_timings:        eval time =   13493.27 ms /   102 runs   (  132.29 ms per token,     7.56 tokens per second)\n",
            "llama_print_timings:       total time =   16521.17 ms /   137 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =       6.26 ms /    33 runs   (    0.19 ms per token,  5271.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2859.00 ms /    41 tokens (   69.73 ms per token,    14.34 tokens per second)\n",
            "llama_print_timings:        eval time =    4167.46 ms /    32 runs   (  130.23 ms per token,     7.68 tokens per second)\n",
            "llama_print_timings:       total time =    7107.49 ms /    73 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =       9.82 ms /    54 runs   (    0.18 ms per token,  5501.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1735.00 ms /    22 tokens (   78.86 ms per token,    12.68 tokens per second)\n",
            "llama_print_timings:        eval time =    6956.06 ms /    53 runs   (  131.25 ms per token,     7.62 tokens per second)\n",
            "llama_print_timings:       total time =    8826.84 ms /    75 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =      26.03 ms /   136 runs   (    0.19 ms per token,  5225.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2354.08 ms /    38 tokens (   61.95 ms per token,    16.14 tokens per second)\n",
            "llama_print_timings:        eval time =   17453.01 ms /   135 runs   (  129.28 ms per token,     7.74 tokens per second)\n",
            "llama_print_timings:       total time =   20154.64 ms /   173 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =       4.58 ms /    24 runs   (    0.19 ms per token,  5235.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1843.30 ms /    29 tokens (   63.56 ms per token,    15.73 tokens per second)\n",
            "llama_print_timings:        eval time =    3069.82 ms /    23 runs   (  133.47 ms per token,     7.49 tokens per second)\n",
            "llama_print_timings:       total time =    4971.88 ms /    52 tokens\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "sentences = {\n",
        "    \"src_snt\":[\n",
        "        \"This paper describes the re-design of the user interface and the interaction for the PDA-based (Personal Digital Assistant) system mediPal.\",\n",
        "        \"The aim with the system is to help people with Parkinson’s disease to a better life by supporting them in their day-to-day struggle with their chronic illness.\",\n",
        "        \"In the paper we discuss the re-design — the process and the resulting user interface.\",\n",
        "        \"The focus is on the evaluation of an earlier version of the system and how that led up to an improved user interface through user-centred systems design (UCSD).\",\n",
        "        \"The paper also discusses how a practitioner can accomplish UCSD in the context of product development and consultant work.\"\n",
        "        ],\n",
        "    \"simp_snt\":[\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\",\n",
        "        \"\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(sentences)\n",
        "df[\"simp_snt\"] = df[\"src_snt\"].apply(simplify)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Sure, I'd be happy to help! Here's a simplified version of the text:\\nResearchers have redesigned the user interface and interactions for a personal digital assistant (PDA) system called mediPal. This system is designed to help people manage their health and wellness by tracking various metrics such as blood pressure, heart rate, and sleep patterns. The new design aims to make it easier and more intuitive for users to access and use the system's features.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"simp_snt\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqHz2PXTjTWc"
      },
      "source": [
        "## Output format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsHX3Qm4dAxb",
        "outputId": "efac7c63-b38a-47ee-9f82-3335cb6533e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../../Models/LLAMA2/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
          ]
        }
      ],
      "source": [
        "llm = Llama(model_path=model_path, n_gpu_layers=-1, chat_format=\"chatml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjlKrx3EdDjr",
        "outputId": "deaf811d-1e5a-4fe4-f3b8-8b4cc4901cf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "from_string grammar:\n",
            "root ::= object \n",
            "object ::= [{] ws object_11 [}] ws \n",
            "value ::= object | array | string | number | value_6 ws \n",
            "array ::= [[] ws array_15 []] ws \n",
            "string ::= [\"] string_18 [\"] ws \n",
            "number ::= number_19 number_25 number_29 ws \n",
            "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
            "ws ::= ws_31 \n",
            "object_8 ::= string [:] ws value object_10 \n",
            "object_9 ::= [,] ws string [:] ws value \n",
            "object_10 ::= object_9 object_10 | \n",
            "object_11 ::= object_8 | \n",
            "array_12 ::= value array_14 \n",
            "array_13 ::= [,] ws value \n",
            "array_14 ::= array_13 array_14 | \n",
            "array_15 ::= array_12 | \n",
            "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
            "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_18 ::= string_16 string_18 | \n",
            "number_19 ::= number_20 number_21 \n",
            "number_20 ::= [-] | \n",
            "number_21 ::= [0-9] | [1-9] number_22 \n",
            "number_22 ::= [0-9] number_22 | \n",
            "number_23 ::= [.] number_24 \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_23 | \n",
            "number_26 ::= [eE] number_27 number_28 \n",
            "number_27 ::= [-+] | \n",
            "number_28 ::= [0-9] number_28 | [0-9] \n",
            "number_29 ::= number_26 | \n",
            "ws_30 ::= [ <U+0009><U+000A>] ws \n",
            "ws_31 ::= ws_30 | \n",
            "\n",
            "\n",
            "llama_print_timings:        load time =    6441.82 ms\n",
            "llama_print_timings:      sample time =    1336.27 ms /   161 runs   (    8.30 ms per token,   120.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6441.37 ms /   110 tokens (   58.56 ms per token,    17.08 tokens per second)\n",
            "llama_print_timings:        eval time =   21638.62 ms /   160 runs   (  135.24 ms per token,     7.39 tokens per second)\n",
            "llama_print_timings:       total time =   29900.00 ms /   270 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8bbd32fd-aefb-4a51-a5ad-d902165489df',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1713879157,\n",
              " 'model': '../../Models/LLAMA2/llama-2-7b-chat.Q4_K_M.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': '{\\n\"difficult_terms\": [\\n{\\n\"term\": \"state-of-the-art performance\",\\n\"definition\": \"The highest level of achievement or accomplishment in a particular field or discipline, as measured by a specific benchmark or metric.\"\\n}, {\\n\"term\": \"Traffic Sign Recognition benchmarks\",\\n\"definition\": \"A set of standards or criteria used to evaluate the performance of a Traffic Sign Recognition system, typically including metrics such as accuracy, precision, and recall.\"\\n}, {\\n\"term\": \"fall short of tackling\",\\n\"definition\": \"To fail to meet or exceed a particular standard or goal, often in relation to a specific task or challenge.\"\\n}\\n]\\n}\\n\\n'},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 110,\n",
              "  'completion_tokens': 160,\n",
              "  'total_tokens': 270}}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c=llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"Provide the definitions of the difficult scientific terms (up to 3) in the following sentence:\\n\\\n",
        "        Though they reach state-of-the-art performance on a particular data-set, but fall short of tackling multiple Traffic Sign Recognition benchmarks.\"},\n",
        "    ],\n",
        "    response_format={\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"term\": {\"definition\": \"string\"}},\n",
        "            \"required\": [\"term\"],\n",
        "        },\n",
        "    },\n",
        "    temperature=0.7,\n",
        ")\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B5flV4QmWjO",
        "outputId": "f8efd400-dc16-45c2-b216-3de0eba0db4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'difficult_terms': [{'term': 'state-of-the-art performance',\n",
              "   'definition': 'The highest level of achievement or accomplishment in a particular field or discipline, as measured by a specific benchmark or metric.'},\n",
              "  {'term': 'Traffic Sign Recognition benchmarks',\n",
              "   'definition': 'A set of standards or criteria used to evaluate the performance of a Traffic Sign Recognition system, typically including metrics such as accuracy, precision, and recall.'},\n",
              "  {'term': 'fall short of tackling',\n",
              "   'definition': 'To fail to meet or exceed a particular standard or goal, often in relation to a specific task or challenge.'}]}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "json.loads(c['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9x8md_gnpQN-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "uRZTwB8QpTeo",
        "outputId": "02e98a6e-0851-4b61-b89c-544655f6ed04"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>snt</th>\n",
              "      <th>term</th>\n",
              "      <th>def</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'state-of-the-art performance', 'defi...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'Traffic Sign Recognition benchmarks'...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'fall short of tackling', 'definition...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 snt  \\\n",
              "0  Though they reach state-of-the-art performance...   \n",
              "1  Though they reach state-of-the-art performance...   \n",
              "2  Though they reach state-of-the-art performance...   \n",
              "\n",
              "                                                term   def  \n",
              "0  {'term': 'state-of-the-art performance', 'defi...  None  \n",
              "1  {'term': 'Traffic Sign Recognition benchmarks'...  None  \n",
              "2  {'term': 'fall short of tackling', 'definition...  None  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.DataFrame({'snt':\"Though they reach state-of-the-art performance on a particular data-set, \\\n",
        "but fall short of tackling multiple Traffic Sign Recognition benchmarks.\",\\\n",
        "                 'term':json.loads(c['choices'][0]['message']['content'])['difficult_terms'],'def':None})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "btwa2rgNreSb",
        "outputId": "2e678ca6-8ce5-4bf4-a192-1fdefaed33c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>snt</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>difficult_terms</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>[{'term': 'state-of-the-art performance', 'def...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               snt  \\\n",
              "difficult_terms  Though they reach state-of-the-art performance...   \n",
              "\n",
              "                                                            output  \n",
              "difficult_terms  [{'term': 'state-of-the-art performance', 'def...  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.DataFrame({'snt':\"Though they reach state-of-the-art performance on a particular data-set, \\\n",
        "but fall short of tackling multiple Traffic Sign Recognition benchmarks.\",\\\n",
        "                 'output':json.loads(c['choices'][0]['message']['content'])})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "oe08uC6irqP8",
        "outputId": "3a964b9b-065d-4846-8be6-aef0b78dc2dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>snt</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>difficult_terms</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'state-of-the-art performance', 'defi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult_terms</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'Traffic Sign Recognition benchmarks'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult_terms</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'fall short of tackling', 'definition...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               snt  \\\n",
              "difficult_terms  Though they reach state-of-the-art performance...   \n",
              "difficult_terms  Though they reach state-of-the-art performance...   \n",
              "difficult_terms  Though they reach state-of-the-art performance...   \n",
              "\n",
              "                                                            output  \n",
              "difficult_terms  {'term': 'state-of-the-art performance', 'defi...  \n",
              "difficult_terms  {'term': 'Traffic Sign Recognition benchmarks'...  \n",
              "difficult_terms  {'term': 'fall short of tackling', 'definition...  "
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=df.explode('output')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "GHusA3abr8Uw",
        "outputId": "0d670f65-cf31-4760-c4cc-0d6c9b130ec1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>snt</th>\n",
              "      <th>output</th>\n",
              "      <th>term</th>\n",
              "      <th>def</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>difficult_terms</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'state-of-the-art performance', 'defi...</td>\n",
              "      <td>state-of-the-art performance</td>\n",
              "      <td>The highest level of achievement or accomplish...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult_terms</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'Traffic Sign Recognition benchmarks'...</td>\n",
              "      <td>Traffic Sign Recognition benchmarks</td>\n",
              "      <td>A set of standards or criteria used to evaluat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult_terms</th>\n",
              "      <td>Though they reach state-of-the-art performance...</td>\n",
              "      <td>{'term': 'fall short of tackling', 'definition...</td>\n",
              "      <td>fall short of tackling</td>\n",
              "      <td>To fail to meet or exceed a particular standar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               snt  \\\n",
              "difficult_terms  Though they reach state-of-the-art performance...   \n",
              "difficult_terms  Though they reach state-of-the-art performance...   \n",
              "difficult_terms  Though they reach state-of-the-art performance...   \n",
              "\n",
              "                                                            output  \\\n",
              "difficult_terms  {'term': 'state-of-the-art performance', 'defi...   \n",
              "difficult_terms  {'term': 'Traffic Sign Recognition benchmarks'...   \n",
              "difficult_terms  {'term': 'fall short of tackling', 'definition...   \n",
              "\n",
              "                                                term  \\\n",
              "difficult_terms         state-of-the-art performance   \n",
              "difficult_terms  Traffic Sign Recognition benchmarks   \n",
              "difficult_terms               fall short of tackling   \n",
              "\n",
              "                                                               def  \n",
              "difficult_terms  The highest level of achievement or accomplish...  \n",
              "difficult_terms  A set of standards or criteria used to evaluat...  \n",
              "difficult_terms  To fail to meet or exceed a particular standar...  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[['term','def']]=df['output'].apply(pd.Series)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qjzVMc8ueSE"
      },
      "source": [
        "## Excercise 2: Difficult terms\n",
        "\n",
        "1.   For 5 sientific sentences in a DataFrame provide up to 3 difficult terms with definitions\n",
        "2.   Save the results into a json format:\n",
        "\n",
        "```\n",
        "{\"snt_id\":\"G14.2_2884788726_2\",\n",
        "\"term\":\"content caching\",\n",
        "\"difficulty\":\"m\",\n",
        "\"term_rank_snt\":1,\n",
        "\"definition\":\"Content caching is a performance optimization mechanism in which data is delivered from the closest servers for optimal application performance.\",\n",
        "\"run_id\":\"UBO_Task2-2_LLAMA2\",\n",
        "\"manual\":0}\n",
        "```\n",
        "\n",
        "where\n",
        "* run_id: Run ID starting with <team_id>_<task_id>_<method_used>, e.g. UBO_Task2-2_LLAMA2\n",
        "* manual: whether the run is manual {0 = no, 1 = yes}\n",
        "* snt_id: a unique passage (sentence) identifier from the input file\n",
        "* term: extracted term\n",
        "* difficulty: difficulty scores of the retrieved term e/m/d (easy/medium/difficult)\n",
        "* term_rank_snt : rank of the term difficulty in the sentence (1 … n).\n",
        "* definition: definition of the term\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYCODx3wzel"
      },
      "source": [
        "#Retrieval-Augmented Generation\n",
        "\n",
        "You may find it beneficial to incorporate factual information into your application. Standard large models can readily access common facts. However, retrieving more specific details or private data may result in the model either indicating it lacks the information or providing inaccurate responses. Retrieval-Augmented Generation (RAG) involves integrating externally sourced information retrieved from a database into the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xf3CGML9x3vY"
      },
      "outputs": [],
      "source": [
        "bip_schedule = {\n",
        "    \"Monday morning\": \"Introduction lecture\",\n",
        "    \"Monday afternoon\": \"API LLAMA Chat\",\n",
        "    \"Tuesday\": \"Evaluation metrics\",\n",
        "    \"Wednesday\":\"RAG\",\n",
        "    \"Thursday\":\"IR\",\n",
        "    \"Friday\":\"projects\"\n",
        "}\n",
        "\n",
        "def complete_and_print(prompt):\n",
        "    return model.create_chat_completion(\n",
        "      messages = [\n",
        "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt\n",
        "          }\n",
        "      ]\n",
        "    )['choices'][0]['message']['content'].strip()\n",
        "\n",
        "\n",
        "def prompt_with_rag(retrived_info, question):\n",
        "    return complete_and_print(\n",
        "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "pEWsMhzy0Avp",
        "outputId": "c8534954-d7c2-45d4-b322-cf90abe6645a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    3716.22 ms\n",
            "llama_print_timings:      sample time =       3.95 ms /    22 runs   (    0.18 ms per token,  5571.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5362.97 ms /    95 tokens (   56.45 ms per token,    17.71 tokens per second)\n",
            "llama_print_timings:        eval time =    2872.50 ms /    21 runs   (  136.79 ms per token,     7.31 tokens per second)\n",
            "llama_print_timings:       total time =    8288.19 ms /   116 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Based on the information provided, the subject on Tuesday morning was \"Evaluation metrics\".'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q=\"What was the subject on Tuesday morning?\"\n",
        "c=prompt_with_rag(bip_schedule,q)\n",
        "c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fCbvuxs1Sb5"
      },
      "source": [
        "### Excercise 3: RAG\n",
        "\n",
        "Provide your own example of RAG, e.g. based on previously shared data or your own data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OohAvB-Y2vl3"
      },
      "source": [
        "#ElasticSearch API\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAIWQhS57zp"
      },
      "source": [
        "The following code queries the DBLP index (digital library in computer science) on the server of the University of Avignon. The collection was indexed by the search engine ElasticSearch and can be queried via the REST API\n",
        "\n",
        "https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html\n",
        "\n",
        "For example, the query to search 10 abstracts on RNN is:\n",
        "\n",
        "https://guacamole.univ-avignon.fr/dblp1/_search?q=RNN&size=10\n",
        "\n",
        "You can relatively easily install ElasticSearch on your server, index and query your collection.\n",
        "\n",
        "Human-friendly GUI interface can be done with Kibana https://www.elastic.co/guide/en/elastic-stack/current/installing-elastic-stack.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXUIzMlv21c3",
        "outputId": "94afaa94-f6f6-4ddf-8cea-1888f10cb164"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "!pip install --upgrade certifi\n",
        "requests.packages.urllib3.disable_warnings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "lbNruGvR42gH"
      },
      "outputs": [],
      "source": [
        "data=pd.DataFrame(columns=['query_text','doc_id','abstract'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "wLEJrbo63s7V",
        "outputId": "2a3ba969-b16d-4534-bd38-732dcc0b7f92"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query_text</th>\n",
              "      <th>doc_id</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>random forest</td>\n",
              "      <td>1472067</td>\n",
              "      <td>This Paper gives an introduction of Random For...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>random forest</td>\n",
              "      <td>2214709255</td>\n",
              "      <td>Decision tree is a simple and effective method...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>random forest</td>\n",
              "      <td>2103321550</td>\n",
              "      <td>The random forest and classification tree mode...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      query_text      doc_id  \\\n",
              "0  random forest     1472067   \n",
              "1  random forest  2214709255   \n",
              "2  random forest  2103321550   \n",
              "\n",
              "                                            abstract  \n",
              "0  This Paper gives an introduction of Random For...  \n",
              "1  Decision tree is a simple and effective method...  \n",
              "2  The random forest and classification tree mode...  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q=\"random forest\"\n",
        "num_retrieved=3\n",
        "try:\n",
        "    a=requests.get(\"https://inex:qatc2011@guacamole.univ-avignon.fr/dblp1/_search?q=\"+q+\"&size=\"+str(num_retrieved),verify=False,auth=('inex','qatc2011'))\n",
        "    a=a.json()['hits']['hits']\n",
        "    for d in a:\n",
        "      ab=d['_source']['abstract']\n",
        "      data=pd.concat([data, pd.DataFrame([{'query_text':q, 'doc_id':d['_id'], 'abstract':ab}])], ignore_index=True)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPJZ7b9T8cdY"
      },
      "source": [
        "### Excercise 4: RAG 2\n",
        "\n",
        "1. Extract the main topics from the top-3 abstracts on a given query\n",
        "2. Provide a one-sentence summary of these abstracts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
